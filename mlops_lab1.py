# -*- coding: utf-8 -*-
"""mlops_lab1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w4wtaTmlzEQjRXuvbNUOj7U8vzvehHWw

# **Sathvika**

# **2022BCS0193**

## **Lab 1: Manual Experiment Tracking Using the Wine Quality Dataset**

**Task 1: Dataset Understanding**
"""

import pandas as pd

df = pd.read_csv("winequality-red.csv", sep=";")

print("Dataset loaded successfully!\n")

num_samples = df.shape[0]
num_features = df.shape[1] - 1
target_variable = "quality"

print("Number of samples:", num_samples)
print("Number of input features:", num_features)
print("Target variable:", target_variable)
print("\nMissing values in each column:")
print(df.isnull().sum())

print("\nData types of features:")
print(df.dtypes)

print("\nDataset Summary:")
print(df.describe())

print("\nQuality score distribution:")
print(df['quality'].value_counts().sort_index())

"""**Task 2: Model Selection**"""

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor

linear_regression_model = LinearRegression()

random_forest_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=15,
    random_state=42
)
print("Linear Model:", linear_regression_model)
print("Tree-based / Ensemble Model:", random_forest_model)

"""**Task 3: Experiment Design**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score

# Load dataset
df = pd.read_csv("winequality-red.csv", sep=";")

X = df.drop("quality", axis=1)
y = df["quality"]

# Utility function to evaluate models
def run_experiment(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    mse = mean_squared_error(y_test, preds)
    r2 = r2_score(y_test, preds)
    return mse, r2

"""## **A. Linear Model Experiments**

**Experiment 1: Baseline Linear Regression**
"""

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

mse_lr1, r2_lr1 = run_experiment(
    LinearRegression(), X_train, X_test, y_train, y_test
)

"""**Experiment 2: Ridge Regression (Hyperparameter change)**"""

mse_lr2, r2_lr2 = run_experiment(
    Ridge(alpha=1.0), X_train, X_test, y_train, y_test
)

"""**Experiment 3: Lasso Regression (Feature selection)**"""

mse_lr3, r2_lr3 = run_experiment(
    Lasso(alpha=0.01), X_train, X_test, y_train, y_test
)

"""**Experiment 4: Standardized Features + Linear Regression (Preprocessing)**"""

pipeline_lr = Pipeline([
    ("scaler", StandardScaler()),
    ("lr", LinearRegression())
])

mse_lr4, r2_lr4 = run_experiment(
    pipeline_lr, X_train, X_test, y_train, y_test
)

"""## **B. Random Forest Experiments**

**Experiment 1: Baseline Random Forest**
"""

rf_base = RandomForestRegressor(
    n_estimators=100,
    random_state=42
)

mse_rf1, r2_rf1 = run_experiment(
    rf_base, X_train, X_test, y_train, y_test
)

"""**Experiment 2: Hyperparameter Change**"""

rf_tuned = RandomForestRegressor(
    n_estimators=200,
    max_depth=10,
    random_state=42
)

mse_rf2, r2_rf2 = run_experiment(
    rf_tuned, X_train, X_test, y_train, y_test
)

"""**Experiment 3: Feature Subset Selection**"""

selected_features = [
    "alcohol", "sulphates", "pH", "volatile acidity"
]

X_subset = X[selected_features]

X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(
    X_subset, y, test_size=0.2, random_state=42
)

rf_subset = RandomForestRegressor(
    n_estimators=100,
    random_state=42
)

mse_rf3, r2_rf3 = run_experiment(
    rf_subset, X_train_s, X_test_s, y_train_s, y_test_s
)

"""**Experiment 4: Different Data Split Strategy**"""

X_train_70, X_test_30, y_train_70, y_test_30 = train_test_split(
    X, y, test_size=0.3, random_state=42
)

rf_split = RandomForestRegressor(
    n_estimators=100,
    random_state=42
)

mse_rf4, r2_rf4 = run_experiment(
    rf_split, X_train_70, X_test_30, y_train_70, y_test_30
)

"""# **Results Summary Table**"""

results = pd.DataFrame({
    "Experiment": [
        "LR Baseline", "Ridge", "Lasso", "Scaled LR",
        "RF Baseline", "RF Tuned", "RF Feature Subset", "RF 70-30 Split"
    ],
    "MSE": [
        mse_lr1, mse_lr2, mse_lr3, mse_lr4,
        mse_rf1, mse_rf2, mse_rf3, mse_rf4
    ],
    "R2 Score": [
        r2_lr1, r2_lr2, r2_lr3, r2_lr4,
        r2_rf1, r2_rf2, r2_rf3, r2_rf4
    ]
})

print(results)

"""## **Task 4: Manual Experiment Tracking**"""

import pandas as pd

experiment_table = pd.DataFrame([
    {
        "Experiment ID": "EXP-01",
        "Model Type": "Linear Regression",
        "Hyperparameters": "Default",
        "Preprocessing Steps": "None",
        "Feature Selection Method": "All features",
        "Train/Test Split": "80/20",
        "MSE": mse_lr1,
        "R² Score": r2_lr1
    },
    {
        "Experiment ID": "EXP-02",
        "Model Type": "Ridge Regression",
        "Hyperparameters": "alpha=1.0",
        "Preprocessing Steps": "None",
        "Feature Selection Method": "All features",
        "Train/Test Split": "80/20",
        "MSE": mse_lr2,
        "R² Score": r2_lr2
    },
    {
        "Experiment ID": "EXP-03",
        "Model Type": "Lasso Regression",
        "Hyperparameters": "alpha=0.01",
        "Preprocessing Steps": "None",
        "Feature Selection Method": "Embedded (L1)",
        "Train/Test Split": "80/20",
        "MSE": mse_lr3,
        "R² Score": r2_lr3
    },
    {
        "Experiment ID": "EXP-04",
        "Model Type": "Linear Regression",
        "Hyperparameters": "Default",
        "Preprocessing Steps": "StandardScaler",
        "Feature Selection Method": "All features",
        "Train/Test Split": "80/20",
        "MSE": mse_lr4,
        "R² Score": r2_lr4
    },
    {
        "Experiment ID": "EXP-05",
        "Model Type": "Random Forest",
        "Hyperparameters": "n_estimators=100",
        "Preprocessing Steps": "None",
        "Feature Selection Method": "All features",
        "Train/Test Split": "80/20",
        "MSE": mse_rf1,
        "R² Score": r2_rf1
    },
    {
        "Experiment ID": "EXP-06",
        "Model Type": "Random Forest",
        "Hyperparameters": "n_estimators=200, max_depth=10",
        "Preprocessing Steps": "None",
        "Feature Selection Method": "All features",
        "Train/Test Split": "80/20",
        "MSE": mse_rf2,
        "R² Score": r2_rf2
    },
    {
        "Experiment ID": "EXP-07",
        "Model Type": "Random Forest",
        "Hyperparameters": "n_estimators=100",
        "Preprocessing Steps": "None",
        "Feature Selection Method": "Manual subset (4 features)",
        "Train/Test Split": "80/20",
        "MSE": mse_rf3,
        "R² Score": r2_rf3
    },
    {
        "Experiment ID": "EXP-08",
        "Model Type": "Random Forest",
        "Hyperparameters": "n_estimators=100",
        "Preprocessing Steps": "None",
        "Feature Selection Method": "All features",
        "Train/Test Split": "70/30",
        "MSE": mse_rf4,
        "R² Score": r2_rf4
    }
])

print(experiment_table)

"""## **Task 5: Analysis and Interpretation**

**1. Which experiment produced the lowest MSE?**

The experiment with the lowest Mean Squared Error (MSE) is:

EXP-05: Random Forest (Baseline)

MSE = 0.301

R² = 0.539

This indicates that the baseline Random Forest model made the most accurate predictions with the least average error.

**2. Which experiment achieved the highest R² score?**

The highest R² score was achieved by:

EXP-05: Random Forest (Baseline)

R² = 0.539

This shows that the model explains approximately 53.9% of the variance in wine quality, which is the best among all experiments.

**3. How did preprocessing affect model performance?**

Preprocessing was applied only in EXP-04 using StandardScaler with Linear Regression.

EXP-01 (No preprocessing): R² = 0.403

EXP-04 (With StandardScaler): R² = 0.403

Observation:
Preprocessing did not improve performance for Linear Regression.

Reason:
Tree-based models do not require feature scaling, and Linear Regression already handles this dataset reasonably without normalization.

**4. Did feature selection improve or degrade performance?**

Feature selection was applied in EXP-07 using a manual subset of 4 features.

EXP-05 (All features): R² = 0.539

EXP-07 (Selected features): R² = 0.455

Observation:
Feature selection degraded performance.

Reason:
Important predictive information was lost by removing features, indicating that most physicochemical attributes contribute to wine quality.

**5. Identify the best overall model with justification**

Best Overall Model:Random Forest Regressor (EXP-05)

Justification:

Achieved lowest MSE (0.301)

Achieved highest R² score (0.539)

Outperformed all linear and regularized models

Demonstrated robustness without requiring heavy preprocessing or tuning

Effectively captured non-linear relationships in the data

Therefore, the baseline Random Forest model is selected as the best overall model for predicting wine quality.

## **Task 6: Reflection**

Structured experiment tracking is critical for scalable and reproducible machine learning workflows because it enables systematic comparison of multiple models, configurations, and preprocessing choices. In this task, tracking experiments helped clearly identify that the baseline Random Forest model outperformed tuned and feature-reduced versions. It ensures that results can be reproduced by recording exact hyperparameters, data splits, and evaluation metrics. Experiment tracking also prevents confusion and redundant experimentation when the number of trials increases. Additionally, it supports informed decision-making by highlighting which changes improve or degrade performance. Overall, structured tracking is essential for maintaining clarity, reliability, and scalability in real-world ML workflows.
"""



